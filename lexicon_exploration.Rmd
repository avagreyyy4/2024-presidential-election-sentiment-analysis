---
title: "lexicon exploration"
output: html_document
date: "2025-02-27"
---
```{r, warning = FALSE, message = FALSE}
library(readr)
library(dplyr)
library(lubridate)
library(ggplot2)
library(tidytext)
library(stringr)
library(textdata)
library(tidyr)
library(scales)
library(forcats)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
```


```{r}
# load the original data set 
full_data <- read.csv("my_data.csv")

# clean the original data set
clean_data <- full_data |> 
  filter(!grepl("France|Sydney|London|Germany|Dubai|Israel|Ontario|Canada|Toronto|Thaiwan|China|UK|Norway|Europe|sydney|ðŸ‡¨ðŸ‡¦|England|India", author.location, ignore.case = TRUE)) |> #remove non-US countries
  filter(grepl("Trump|Donald|Harris|Kamala", fullText, ignore.case = TRUE)) |>  # keep only tweets mentioning either candidate
  filter(!grepl("-\\s*Kamala Harris", fullText)) |> # remove quotes by either canidate
  filter(!grepl("-\\s*Donald Trump", fullText)) |>
  mutate(fullText = tolower(fullText)) |>  #convert all text to lower case
  mutate(fullText = gsub("@\\w+", "", fullText)) |>  # remove mentions
  mutate(fullText = gsub("#\\w+", "", fullText)) |>  # remove hashtags
  mutate(fullText = gsub("[^[:alnum:]' ]", "", fullText)) |>
  mutate(fullText = gsub("\\s+", " ", fullText)) |>   
  distinct(fullText, author.id, .keep_all = TRUE) |> # remove duplicates 
  distinct(fullText, .keep_all = TRUE) 
```


```{r}
# further cleaning to add months and specific candidates mentioned
valid_months <- month.abb  # built in abbreviation month string

clean_data <- clean_data |> 
  mutate(month = str_extract(createdAt, paste(valid_months, collapse = "|")))

# Define patterns to match variations of each candidate's name
trump_pattern <- regex("(trump|donald)", ignore_case = TRUE)
harris_pattern <- regex("(kamala|harris)", ignore_case = TRUE)

# Create a new column 'candidate_mentioned' based on the patterns
clean_data <- clean_data |>
  mutate(candidate_mentioned = case_when(
    str_detect(fullText, trump_pattern) & str_detect(fullText, harris_pattern) ~ "Trump, Harris",
    str_detect(fullText, trump_pattern) ~ "Trump",
    str_detect(fullText, harris_pattern) ~ "Harris"
  ))

#further cleaning to remove extra tweets not mentioning either candidate
clean_data <- clean_data |> filter(!is.na(candidate_mentioned))
```

```{r}
clean_data <- clean_data |> 
  mutate(day = day(as.POSIXct(createdAt, format="%a %b %d %H:%M:%S %z %Y", tz="UTC")))
```



```{r}
#tokenize the data
token_tweets <- clean_data |>
  unnest_tokens(word, fullText)
# count the individual word count with no restrictions
overall_count <- token_tweets |>
  count(word, sort = TRUE)

head(overall_count, 15) # again, just showing overall values
```

Utilizing two lexicons to encompas both emotion and sentiment analysis
- VADER for microblogging sentiment analysis
- NRC for emotion mining


```{r}
#Now, working with data to explore candidate specific trends
candidate_name <- c("Harris", "Trump", "Trump, Harris")

#Group my candidate to be able to analyze specific Trump or Harris Tweets and emotional trends
tidy_candidates <- clean_data |>
  group_by(candidate_mentioned) |>
  mutate(detection = cumsum(str_detect(fullText, str_c(candidate_name, collapse = "|")))) |>  # Fix parentheses
  ungroup() |>
  unnest_tokens(word, fullText)

months <- c("Jul", "Aug", "Sep", "Oct", "Nov")
tidy_month <- clean_data |>
  group_by(month) |>
  mutate(detection = cumsum(str_detect(fullText, str_c(months, collapse = "|")))) |>  # Fix parentheses
  ungroup() |>
  unnest_tokens(word, fullText)
```

```{r}
emotion_trends <- function(candidate, emotion){
  nrc_emoton <- get_sentiments("nrc")|>
    filter(sentiment == emotion)
  
  candidate_emotion <- tidy_candidates |>
    filter(candidate_mentioned == candidate)|>
    inner_join(nrc_emoton) |>
    count(word, sort = TRUE)
  
  return(candidate_emotion)
}

```


```{r}
#fear
emotion_trends("Trump", "fear")
emotion_trends("Harris", "fear")
```

```{r}
emotion_trends("Trump", "anger")
emotion_trends("Harris", "anger")
```

```{r}
emotion_trends("Trump", "disgust")
emotion_trends("Harris", "disgust")
```

```{r}
emotion_trends("Trump", "trust")
emotion_trends("Harris", "trust")
```

```{r}
emotion_trends("Trump", "joy")
emotion_trends("Harris", "joy")
```

```{r}
emotion_trends("Trump", "anticipation")
emotion_trends("Harris", "anticipation")
```



*
Deleted the monthly sentiment scores since they do not encompass well based on individual word usage
*

```{r}
vader_data <- read_csv("VADER.csv", col_names = FALSE)

vader_data <- vader_data |> 
  rename(word = `X1`,
         sentiment_score = `X2`,
         standard_deviation = `X3`,
         individual_score = `X4`) |>
  slice(442:7504)|>
  select(1:2) |>
  mutate(sentiment = case_when(
    sentiment_score < 0 ~ "Negative",
    sentiment_score >0 ~ "Positive"
  ))

head(vader_data, 10)
```



```{r}
nrc_word_count_trump <- tidy_candidates |> 
  filter(word != "trump", word != "vote") |>  
  inner_join(get_sentiments("nrc"), by = "word") |>  
  count(candidate_mentioned, word, sentiment, sort = TRUE) |>  # Keep candidate_mentioned
  ungroup()|>
  filter(candidate_mentioned == "Trump")

nrc_word_count_harris <- tidy_candidates |> 
  filter(word != "trump", word != "vote") |>  
  inner_join(get_sentiments("nrc"), by = "word") |>  
  count(candidate_mentioned, word, sentiment, sort = TRUE) |>  # Keep candidate_mentioned
  ungroup()|>
  filter(candidate_mentioned == "Harris")

vader_word_count_trump <- tidy_candidates |>
  filter(word != "trump", word != "vote") |>  
  inner_join(vader_data, by = "word") |>  
  count(candidate_mentioned, word, sentiment, sort = TRUE) |>  # Keep candidate_mentioned
  ungroup()|>
  filter(candidate_mentioned == "Trump")

vader_word_count_harris <- tidy_candidates |>
  filter(word != "trump", word != "vote") |>  
  inner_join(vader_data, by = "word") |>  
  count(candidate_mentioned, word, sentiment, sort = TRUE) |>  # Keep candidate_mentioned
  ungroup()|>
  filter(candidate_mentioned == "Harris")
```



```{r}
nrc_word_count_trump |>
  group_by(sentiment) |>
  top_n(10) |>
  ungroup()|>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment TRUMP NRC",
       x = NULL) +
  coord_flip()

nrc_word_count_harris |>
  group_by(sentiment) |>
  top_n(10) |>
  ungroup()|>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment HARRIS NRC",
       x = NULL) +
  coord_flip()

vader_word_count_trump |> 
  group_by(sentiment) |>
  top_n(10) |>
  ungroup()|>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment TRUMP NRC",
       x = NULL) +
  coord_flip()

vader_word_count_harris |> 
  group_by(sentiment) |>
  top_n(10) |>
  ungroup()|>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment HARRIS NRC",
       x = NULL) +
  coord_flip()


```

```{r}
word_cloud_candidate <- function(candidate){
  tidy_candidate <- tidy_candidates |>
    filter(candidate_mentioned == candidate)|>
    filter(word != "trump")|>
    inner_join(vader_data) |>
    count(word, sentiment, sort = TRUE) |>
    acast(word ~ sentiment, value.var = "n", fill = 0) |>
    comparison.cloud(colors = c("darkred", "forestgreen"), max.words = 50)
}

word_cloud_candidate("Trump")
word_cloud_candidate("Harris")
```

```{r}
election_sentences <- clean_data |>
  unnest_tokens(sentence, fullText, token = "sentences")

vadersentiment <- vader_data|>
    filter(sentiment == "Positive") 
  
word_count <- tidy_candidates |>
    group_by(candidate_mentioned, month)|>
    summarize(words = n())
  
tidy_candidates |> 
    semi_join(vadersentiment)|>
    group_by(candidate_mentioned, month)|>
    summarize(sentimentwords = n())|>
    left_join(word_count, by = c("candidate_mentioned", "month")) |>
    mutate(ratio = sentimentwords/words) |>
    filter(candidate_mentioned != "Trump, Harris")|>
    top_n(1)|>
    ungroup()


```

#check frequency for sentiment rather than emotion

```{r}
token_tweets_sentiment <- token_tweets |> 
  inner_join(vader_data, by = "word")

sentiment_2 <- token_tweets_sentiment |> filter(candidate_mentioned != "Trump, Harris")

freq <- sentiment_2 |>
  count(candidate_mentioned, word) |> 
  group_by(candidate_mentioned) |> 
  mutate(proportion = n / sum(n)) |> 
  select(-n) |> 
  pivot_wider(names_from = candidate_mentioned, values_from = proportion, values_fill = 0)

cor.test(freq$Harris, freq$Trump)
```


#iNVERSE TERM FREQUENCY (beginning feature engineering)

```{r}
candidate_words <- clean_data |>
  unnest_tokens(word, fullText) |>
  count(candidate_mentioned, word, sort = TRUE) |>
  ungroup()

total_words <- candidate_words |>
  group_by(candidate_mentioned) |>
  summarize(total = sum(n))

candidate_words <- left_join(candidate_words, total_words)

candidate_words

ggplot(candidate_words, aes(n/total, fill = candidate_mentioned)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.00025) + 
  facet_wrap(~ candidate_mentioned, ncol = 2, scales = "free_y")
```
## zipf law = frequency of a word is inversley related to its rank

```{r}
freq_by_rank <- candidate_words |>
  group_by(candidate_mentioned) |>
  mutate(rank = row_number(),
         term_frequency = n/total)

freq_by_rank
```

```{r}
freq_by_rank |>
  ggplot(aes(rank, term_frequency, color = candidate_mentioned)) +
  geom_line(linewidth = 1.1, alpha = 0.8, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()
```

```{r}
#not entirely constant so lets look at the middle section

rank_subset <- freq_by_rank |>
  filter(rank < 500,
         rank > 10)

lm(log10(term_frequency) ~ log10(rank), data = rank_subset)
```

```{r}
freq_by_rank |>
  ggplot(aes(rank, term_frequency, color = candidate_mentioned)) + 
  geom_abline(intercept = -0.68, slope = -1.09, color = "black", linetype =2) +
  geom_line(linewidth = 1.1, alpha = 0.8, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()
```

```{r}
candidate_words <- candidate_words |>
  bind_tf_idf(word, candidate_mentioned, n)

candidate_words
#makes sense how stop words have values of zero but then we see candidate names have values since they are far more frequent in specific tweets than others

```
```{r}
#since only 3 groups a lot of values may be close together 
candidate_words |>
  arrange(desc(tf_idf)) |>
  mutate(word = factor(word, levels = rev(unique(word))))|>
  group_by(candidate_mentioned)|>
  top_n(15) |>
  ungroup() |>
  ggplot(aes(word, tf_idf, fill = candidate_mentioned)) +
  geom_col(show.legend = FALSE) + 
  labs(x = NULL, y = "tf-idf") + 
  facet_wrap(~candidate_mentioned, ncol = 2, scales = "free") +
  coord_flip()
```

#moving into n grams/bi grams

```{r}
clean_trump <- clean_data |> filter(candidate_mentioned == "Trump")


trump_bigrams <- clean_trump |>
  unnest_tokens(trump_bigram, fullText, token = "ngrams", n =2)

trump_bigrams |> count(trump_bigram, sort = TRUE)

clean_harris <- clean_data |> filter(candidate_mentioned == "Harris")


harris_bigrams <- clean_harris |>
  unnest_tokens(harris_bigram, fullText, token = "ngrams", n =2)

harris_bigrams |> count(harris_bigram, sort = TRUE)
```

```{r}
stop_words <- stop_words |> filter(lexicon != "SMART")
trump_bigrams_separated <- trump_bigrams |>
  separate(trump_bigram, c("word1", "word2"), sep = " ")

bigrams_filtered_trump <- trump_bigrams_separated |>
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word)

bigram_trump_count <- bigrams_filtered_trump |>
  count(word1, word2, sort = TRUE)

bigram_trump_count
```

```{r}
stop_words <- stop_words |> filter(lexicon != "SMART")
harris_bigrams_separated <- harris_bigrams |>
  separate(harris_bigram, c("word1", "word2"), sep = " ")

bigrams_filtered_harris <- harris_bigrams_separated |>
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word)

bigram_harris_count <- bigrams_filtered_harris |>
  count(word1, word2, sort = TRUE)

bigram_harris_count
```



