---
title: "lexicon exploration"
output: html_document
date: "2025-02-27"
---
```{r, warning = FALSE, message = FALSE}
library(readr)
library(dplyr)
library(lubridate)
library(ggplot2)
library(tidytext)
library(stringr)
library(textdata)
library(tidyr)
library(scales)
library(forcats)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(igraph)
```


```{r}
# load the original data set 
full_data <- read.csv("my_data.csv")
#/Users/avagrey/Desktop/

# clean the original data set
clean_data <- full_data |> 
  filter(!grepl("France|Sydney|London|Germany|Dubai|Israel|Ontario|Canada|Toronto|Thaiwan|China|UK|Norway|Europe|sydney|ðŸ‡¨ðŸ‡¦|England|India", author.location, ignore.case = TRUE)) |> #remove non-US countries
  filter(grepl("Trump|Donald|Harris|Kamala", fullText, ignore.case = TRUE)) |>  # keep only tweets mentioning either candidate
  filter(!grepl("-\\s*Kamala Harris", fullText)) |> # remove quotes by either canidate
  filter(!grepl("-\\s*Donald Trump", fullText)) |>
  mutate(fullText = tolower(fullText)) |>  #convert all text to lower case
  mutate(fullText = gsub("@\\w+", "", fullText)) |>  # remove mentions
  mutate(fullText = gsub("#\\w+", "", fullText)) |>  # remove hashtags
  mutate(fullText = gsub("[^[:alnum:]' ]", "", fullText)) |>
  mutate(fullText = gsub("\\s+", " ", fullText)) |>   
  distinct(fullText, author.id, .keep_all = TRUE) |> # remove duplicates 
  distinct(fullText, .keep_all = TRUE) 
```


```{r}
# further cleaning to add months and specific candidates mentioned
valid_months <- month.abb  # built in abbreviation month string

clean_data <- clean_data |> 
  mutate(month = str_extract(createdAt, paste(valid_months, collapse = "|")))

# Define patterns to match variations of each candidate's name
trump_pattern <- regex("(trump|donald)", ignore_case = TRUE)
harris_pattern <- regex("(kamala|harris)", ignore_case = TRUE)

# Create a new column 'candidate_mentioned' based on the patterns
clean_data <- clean_data |>
  mutate(candidate_mentioned = case_when(
    str_detect(fullText, trump_pattern) & str_detect(fullText, harris_pattern) ~ "Trump, Harris",
    str_detect(fullText, trump_pattern) ~ "Trump",
    str_detect(fullText, harris_pattern) ~ "Harris"
  ))

#further cleaning to remove extra tweets not mentioning either candidate
clean_data <- clean_data |> filter(!is.na(candidate_mentioned))
```

```{r}
clean_data <- clean_data |> 
  mutate(day = day(as.POSIXct(createdAt, format="%a %b %d %H:%M:%S %z %Y", tz="UTC")))
```



```{r}
#tokenize the data
token_tweets <- clean_data |>
  unnest_tokens(word, fullText)
# count the individual word count with no restrictions
overall_count <- token_tweets |>
  count(word, sort = TRUE)

head(overall_count, 15) # again, just showing overall values
```

Utilizing two lexicons to encompas both emotion and sentiment analysis
- VADER for microblogging sentiment analysis
- NRC for emotion mining


```{r}
#Now, working with data to explore candidate specific trends
candidate_name <- c("Harris", "Trump", "Trump, Harris")

#Group my candidate to be able to analyze specific Trump or Harris Tweets and emotional trends
tidy_candidates <- clean_data |>
  group_by(candidate_mentioned) |>
  mutate(detection = cumsum(str_detect(fullText, str_c(candidate_name, collapse = "|")))) |>  # Fix parentheses
  ungroup() |>
  unnest_tokens(word, fullText)

months <- c("Jul", "Aug", "Sep", "Oct", "Nov")
tidy_month <- clean_data |>
  group_by(month) |>
  mutate(detection = cumsum(str_detect(fullText, str_c(months, collapse = "|")))) |>  # Fix parentheses
  ungroup() |>
  unnest_tokens(word, fullText)
```

```{r}
emotion_trends <- function(candidate, emotion){
  nrc_emoton <- get_sentiments("nrc")|>
    filter(sentiment == emotion)
  
  candidate_emotion <- tidy_candidates |>
    filter(candidate_mentioned == candidate)|>
    inner_join(nrc_emoton) |>
    count(word, sort = TRUE)
  
  return(candidate_emotion)
}

```


```{r}
#fear
emotion_trends("Trump", "fear")
emotion_trends("Harris", "fear")
```

```{r}
emotion_trends("Trump", "anger")
emotion_trends("Harris", "anger")
```

```{r}
emotion_trends("Trump", "disgust")
emotion_trends("Harris", "disgust")
```

```{r}
emotion_trends("Trump", "trust")
emotion_trends("Harris", "trust")
```

```{r}
emotion_trends("Trump", "joy")
emotion_trends("Harris", "joy")
```

```{r}
emotion_trends("Trump", "anticipation")
emotion_trends("Harris", "anticipation")
```



*
Deleted the monthly sentiment scores since they do not encompass well based on individual word usage
*

```{r}
vader_data <- read_csv("VADER.csv", col_names = FALSE)

vader_data <- vader_data |> 
  rename(word = `X1`,
         sentiment_score = `X2`,
         standard_deviation = `X3`,
         individual_score = `X4`) |>
  slice(442:7504)|>
  select(1:2) |>
  mutate(sentiment = case_when(
    sentiment_score < 0 ~ "Negative",
    sentiment_score >0 ~ "Positive"
  ))

head(vader_data, 10)
```



```{r}
nrc_word_count_trump <- tidy_candidates |> 
  filter(word != "trump", word != "vote") |>  
  inner_join(get_sentiments("nrc"), by = "word") |>  
  count(candidate_mentioned, word, sentiment, sort = TRUE) |>  # Keep candidate_mentioned
  ungroup()|>
  filter(candidate_mentioned == "Trump")

nrc_word_count_harris <- tidy_candidates |> 
  filter(word != "trump", word != "vote") |>  
  inner_join(get_sentiments("nrc"), by = "word") |>  
  count(candidate_mentioned, word, sentiment, sort = TRUE) |>  # Keep candidate_mentioned
  ungroup()|>
  filter(candidate_mentioned == "Harris")

vader_word_count_trump <- tidy_candidates |>
  filter(word != "trump", word != "vote") |>  
  inner_join(vader_data, by = "word") |>  
  count(candidate_mentioned, word, sentiment, sort = TRUE) |>  # Keep candidate_mentioned
  ungroup()|>
  filter(candidate_mentioned == "Trump")

vader_word_count_harris <- tidy_candidates |>
  filter(word != "trump", word != "vote") |>  
  inner_join(vader_data, by = "word") |>  
  count(candidate_mentioned, word, sentiment, sort = TRUE) |>  # Keep candidate_mentioned
  ungroup()|>
  filter(candidate_mentioned == "Harris")
```



```{r}
nrc_word_count_trump |>
  group_by(sentiment) |>
  top_n(10) |>
  ungroup()|>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment TRUMP NRC",
       x = NULL) +
  coord_flip()

nrc_word_count_harris |>
  group_by(sentiment) |>
  top_n(10) |>
  ungroup()|>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment HARRIS NRC",
       x = NULL) +
  coord_flip()

vader_word_count_trump |> 
  group_by(sentiment) |>
  top_n(10) |>
  ungroup()|>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment TRUMP NRC",
       x = NULL) +
  coord_flip()

vader_word_count_harris |> 
  group_by(sentiment) |>
  top_n(10) |>
  ungroup()|>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment HARRIS NRC",
       x = NULL) +
  coord_flip()


```

```{r}
word_cloud_candidate <- function(candidate){
  tidy_candidate <- tidy_candidates |>
    filter(candidate_mentioned == candidate)|>
    filter(word != "trump")|>
    inner_join(vader_data) |>
    count(word, sentiment, sort = TRUE) |>
    acast(word ~ sentiment, value.var = "n", fill = 0) |>
    comparison.cloud(colors = c("darkred", "forestgreen"), max.words = 50)
}

word_cloud_candidate("Trump")
word_cloud_candidate("Harris")
```

```{r}
election_sentences <- clean_data |>
  unnest_tokens(sentence, fullText, token = "sentences")

vadersentiment <- vader_data|>
    filter(sentiment == "Positive") 
  
word_count <- tidy_candidates |>
    group_by(candidate_mentioned, month)|>
    summarize(words = n())
  
tidy_candidates |> 
    semi_join(vadersentiment)|>
    group_by(candidate_mentioned, month)|>
    summarize(sentimentwords = n())|>
    left_join(word_count, by = c("candidate_mentioned", "month")) |>
    mutate(ratio = sentimentwords/words) |>
    filter(candidate_mentioned != "Trump, Harris")|>
    top_n(1)|>
    ungroup()


```

#check frequency for sentiment rather than emotion

```{r}
token_tweets_sentiment <- token_tweets |> 
  inner_join(vader_data, by = "word")

sentiment_2 <- token_tweets_sentiment |> filter(candidate_mentioned != "Trump, Harris")

freq <- sentiment_2 |>
  count(candidate_mentioned, word) |> 
  group_by(candidate_mentioned) |> 
  mutate(proportion = n / sum(n)) |> 
  select(-n) |> 
  pivot_wider(names_from = candidate_mentioned, values_from = proportion, values_fill = 0)

cor.test(freq$Harris, freq$Trump)
```


#iNVERSE TERM FREQUENCY (beginning feature engineering)

```{r}
candidate_words <- clean_data |>
  unnest_tokens(word, fullText) |>
  count(candidate_mentioned, word, sort = TRUE) |>
  ungroup()

total_words <- candidate_words |>
  group_by(candidate_mentioned) |>
  summarize(total = sum(n))

candidate_words <- left_join(candidate_words, total_words)

candidate_words

ggplot(candidate_words, aes(n/total, fill = candidate_mentioned)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.00025) + 
  facet_wrap(~ candidate_mentioned, ncol = 2, scales = "free_y")
```
## zipf law = frequency of a word is inversley related to its rank

```{r}
freq_by_rank <- candidate_words |>
  group_by(candidate_mentioned) |>
  mutate(rank = row_number(),
         term_frequency = n/total)

freq_by_rank
```

```{r}
freq_by_rank |>
  ggplot(aes(rank, term_frequency, color = candidate_mentioned)) +
  geom_line(linewidth = 1.1, alpha = 0.8, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()
```

```{r}
#not entirely constant so lets look at the middle section

rank_subset <- freq_by_rank |>
  filter(rank < 500,
         rank > 10)

lm(log10(term_frequency) ~ log10(rank), data = rank_subset)
```

```{r}
freq_by_rank |>
  ggplot(aes(rank, term_frequency, color = candidate_mentioned)) + 
  geom_abline(intercept = -0.68, slope = -1.09, color = "black", linetype =2) +
  geom_line(linewidth = 1.1, alpha = 0.8, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()
```

```{r}
candidate_words <- candidate_words |>
  bind_tf_idf(word, candidate_mentioned, n)

candidate_words
#makes sense how stop words have values of zero but then we see candidate names have values since they are far more frequent in specific tweets than others

```
```{r}
#since only 3 groups a lot of values may be close together 
candidate_words |>
  arrange(desc(tf_idf)) |>
  mutate(word = factor(word, levels = rev(unique(word))))|>
  group_by(candidate_mentioned)|>
  top_n(15) |>
  ungroup() |>
  ggplot(aes(word, tf_idf, fill = candidate_mentioned)) +
  geom_col(show.legend = FALSE) + 
  labs(x = NULL, y = "tf-idf") + 
  facet_wrap(~candidate_mentioned, ncol = 2, scales = "free") +
  coord_flip()
```

#moving into n grams/bi grams

```{r}
clean_trump <- clean_data |> filter(candidate_mentioned == "Trump")


trump_bigrams <- clean_trump |>
  unnest_tokens(bigrams, fullText, token = "ngrams", n =2)

trump_bigrams |> count(bigrams, sort = TRUE)

clean_harris <- clean_data |> filter(candidate_mentioned == "Harris")


harris_bigrams <- clean_harris |>
  unnest_tokens(bigrams, fullText, token = "ngrams", n =2)

harris_bigrams |> count(bigrams, sort = TRUE)
```

```{r}
stop_words <- stop_words |> filter(lexicon != "SMART") |> filter(word != "not")
```

```{r}
trump_bigrams_separated <- trump_bigrams |>
  separate(bigrams, c("word1", "word2"), sep = " ")

bigrams_filtered_trump <- trump_bigrams_separated |>
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word)

bigram_trump_count <- bigrams_filtered_trump |>
  count(word1, word2, sort = TRUE)

bigram_trump_count
```

```{r}
harris_bigrams_separated <- harris_bigrams |>
  separate(bigrams, c("word1", "word2"), sep = " ")

bigrams_filtered_harris <- harris_bigrams_separated |>
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word)

bigram_harris_count <- bigrams_filtered_harris |>
  count(word1, word2, sort = TRUE)

bigram_harris_count
```

#tri gram
```{r}
trump_trigrams <- clean_trump |>
  unnest_tokens(trump_trigram, fullText, token = "ngrams", n =3)

trump_trigrams |> count(trump_trigram, sort = TRUE)


harris_trigrams <- clean_harris |>
  unnest_tokens(harris_trigram, fullText, token = "ngrams", n =3)

harris_trigrams |> count(harris_trigram, sort = TRUE)

#it seems kamala harris is is a lot more common while trump has more opionated statements "i think.."
```

```{r}
trump_trigrams_separated <- trump_trigrams |>
  separate(trump_trigram, c("word1", "word2", "word3"), sep = " ")

trigrams_filtered_trump <- trump_trigrams_separated |>
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word) |>
  filter(!word3 %in% stop_words$word)

trigram_trump_count <- trigrams_filtered_trump |>
  count(word1, word2, word3, sort = TRUE)

trigram_trump_count

```

```{r}
harris_trigrams_separated <- harris_trigrams |>
  separate(harris_trigram, c("word1", "word2", "word3"), sep = " ")

trigrams_filtered_harris <- harris_trigrams_separated |>
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word) |>
  filter(!word3 %in% stop_words$word)

trigram_harris_count <- trigrams_filtered_harris |>
  count(word1, word2, word3, sort = TRUE)

trigram_harris_count
```

#4-gram
```{r}
clean_trump_tokens <- clean_trump |>
  unnest_tokens(word, fullText) |>
  filter(!word %in% stop_words$word)

clean_trump_rejoined <- clean_trump_tokens |>
  group_by(X) |>  
  summarize(clean_text = paste(word, collapse = " "), .groups = "drop")

trump_fourgrams <- clean_trump_rejoined |>
  unnest_tokens(trump_fourgram, clean_text, token = "ngrams", n=3)

trump_fourgrams |> count(trump_fourgram, sort = TRUE)

clean_harris_tokens <- clean_harris |>
  unnest_tokens(word, fullText) |>
  filter(!word %in% stop_words$word)

clean_harris_rejoined <- clean_harris_tokens |>
  group_by(X) |>  
  summarize(clean_text = paste(word, collapse = " "), .groups = "drop")

harris_fourgrams <- clean_harris_rejoined |>
  unnest_tokens(harris_fourgram, clean_text, token = "ngrams", n=3)

harris_fourgrams |> count(harris_fourgram, sort = TRUE)
```


#examining bigrams further
```{r}
trigrams_filtered_trump |> filter(word2 == "support") |>
  count(word1, word3, sort = TRUE)

trigrams_filtered_harris |> filter(word2 == "support") |>
  count(word1, word3, sort = TRUE)
```

```{r}
bigrams_filtered_trump |> filter(word1 == "honestly" | word2 == "honestly") |>
  count(word1, word2, sort = TRUE)

bigrams_filtered_harris |> filter(word1 == "honestly" | word2 == "honestly") |>
  count(word1, word2, sort = TRUE)
```

```{r}
bigrams_filtered_trump |> filter(word1 == "bad" | word2 == "bad") |>
  count(word1, word2, sort = TRUE)

bigrams_filtered_harris |> filter(word1 == "bad" | word2 == "bad") |>
  count(word1, word2, sort = TRUE)
```


```{r}
bigrams_united_trump <- bigrams_filtered_trump |>
  unite(bigrams, word1, word2, sep = " ") 

bigrams_united_harris <- bigrams_filtered_harris |>
  unite(bigrams, word1, word2, sep = " ") 

bigrams_united <- bind_rows(bigrams_united_trump, bigrams_united_harris)

bigrams_counts <- bind_rows(bigram_trump_count, bigram_harris_count)

bigram_tf_idf<- bigrams_united|>
  count(candidate_mentioned, bigrams) |>
  bind_tf_idf(candidate_mentioned, bigrams, n) |>
  arrange(desc(tf_idf))

bigram_tf_idf
```


```{r}
bigram_tf_idf |>
  group_by(candidate_mentioned) |>
  arrange(desc(tf_idf), desc(n)) |>  # Sort by TF-IDF and frequency
  slice_head(n = 10) |>
  ungroup() |>
  mutate(bigrams = reorder(bigrams, tf_idf)) |>
  ggplot(aes(bigrams, tf_idf, fill = candidate_mentioned)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~candidate_mentioned, scales = "free_y") +
  labs(y = "TF-IDF for each candidate",
       x = NULL) +
  coord_flip()

#not helpul since so much overlap 
```

#using bigrams to contexualize 
```{r}
bigrams_separated <- bigrams_united |>
  separate(bigrams, c("word1", "word2"), sep = " ")


bigrams_separated|>
  group_by(candidate_mentioned)|>
  filter(word1 == "not") |>
  count(word1, word2, sort = TRUE)
```
Negation word impact

```{r}
not_vader <- bigrams_separated|>
  filter(word1 == "not") |>
  inner_join(vader_data, by = c(word2 = "word"))|>
  count(word2, sentiment_score, sort = TRUE)|>
  ungroup() 

not_vader

not_vader |>
  mutate(contribution = n*sentiment_score) |>
  arrange(desc(abs(contribution))) |>
  head(20) |>
  mutate(word2 = reorder(word2, contribution)) |>
  ggplot(aes(word2, n*sentiment_score, fill = n*sentiment_score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Word preceded by \"not\"") +
  ylab("Sentiment score times number of occurances") +
  coord_flip()
```


```{r} 
#not working
not_vader2 <- bigrams_separated|>
  filter(word1 == "no") |>
  inner_join(vader_data, by = c(word2 = "word"))|>
  count(word2, sentiment_score, sort = TRUE)|>
  ungroup() 

not_vader2

#not_vader2 |>
#  mutate(contribution = n*sentiment_score) |>
#  arrange(desc(abs(contribution))) |>
#  head(20) |>
#  mutate(word2 = reorder(word2, contribution)) |>
#  ggplot(aes(word2, n*sentiment_score, fill = n*sentiment_score > 0)) +
#  geom_col(show.legend = FALSE) +
#  xlab("Word preceded by \"not\"") +
#  ylab("Sentiment score times number of occurances") +
#  coord_flip()
```

```{r}
bigrams_separated
```


```{r}
trump_graph <- bigram_trump_count |> 
  filter(n > 20) |>  
  graph_from_data_frame(directed = TRUE) 

trump_graph
```

```{r}
set.seed(2025)

b <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(trump_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = b, end_cap = circle(0.07, 'inches')) +
  geom_node_point(color = "red", size = 2.5)+
  geom_node_text(aes(label = name, vjust = 1, hjust = 1)) +
  theme_void()
  
```

```{r}
harris_graph <- bigram_harris_count |> 
  filter(n > 20) |>  
  graph_from_data_frame(directed = TRUE) 

harris_graph

set.seed(2025)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(harris_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(0.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 2.5)+
  geom_node_text(aes(label = name, vjust = 1, hjust = 1)) +
  theme_void()
```

```{r}
#trump_section_words <- clean_data |>
  #unnest_tokens(word, fullText)|>
 # filter(!word %in% stop_words$word)

#word_pairs_trump <- trump_section_words |>
  #pairwise_count(word, candidate_mentioned, sort = TRUE)
```


