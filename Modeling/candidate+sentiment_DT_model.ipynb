{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0137aa03-124a-402c-90bd-907bfce9c125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/avagrey/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################\n",
    "# STEP 0: import libraries\n",
    "########################################\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.tree\n",
    "import sklearn.ensemble\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import sklearn.datasets\n",
    "import sklearn.decomposition\n",
    "import sklearn.discriminant_analysis\n",
    "import sklearn.ensemble\n",
    "import sklearn.linear_model\n",
    "import sklearn.neural_network\n",
    "import sklearn.model_selection\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.neighbors\n",
    "import sklearn.preprocessing\n",
    "import sklearn.random_projection\n",
    "import sklearn.tree\n",
    "import sklearn.svm\n",
    "nltk.download('vader_lexicon') # Should print 3.9.1 or another recent version\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "f7937815-0dda-491a-a63d-f7640bca01c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>fullText</th>\n",
       "      <th>candidate_mentioned</th>\n",
       "      <th>day</th>\n",
       "      <th>July</th>\n",
       "      <th>August</th>\n",
       "      <th>September</th>\n",
       "      <th>October</th>\n",
       "      <th>November</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>views</th>\n",
       "      <th>comments</th>\n",
       "      <th>engagement_rate</th>\n",
       "      <th>id</th>\n",
       "      <th>Candidate</th>\n",
       "      <th>direction</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6232</td>\n",
       "      <td>i think what people aren’t taking about enough...</td>\n",
       "      <td>Trump</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.020858</td>\n",
       "      <td>1</td>\n",
       "      <td>trump</td>\n",
       "      <td>indirect</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      X                                           fullText  \\\n",
       "0  6232  i think what people aren’t taking about enough...   \n",
       "\n",
       "  candidate_mentioned  day  July  August  September  October  November  \\\n",
       "0               Trump   14     0       1          0        0         0   \n",
       "\n",
       "      likes  retweets     views  comments  engagement_rate  id Candidate  \\\n",
       "0  0.000025   0.00001  0.000023  0.000019         0.020858   1     trump   \n",
       "\n",
       "  direction Sentiment  \n",
       "0  indirect  negative  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###reading in initial###\n",
    "df = pd.read_csv((\"~/Downloads/train_data.csv\"))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "121b2d9d-e8f8-46d7-828c-5ff78e9edf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER matches manual labels 45.80% of the time.\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# SUB-STEP: Prepare the dataset\n",
    "########################################\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_vader_score(text):\n",
    "    return analyzer.polarity_scores(text)['compound']\n",
    "\n",
    "df['vader_score'] = df['fullText'].apply(get_vader_score)\n",
    "\n",
    "\n",
    "def sentiment_match(vader_score, label):\n",
    "    if vader_score > 0.05:\n",
    "        vader_label = \"positive\"\n",
    "    elif vader_score < -0.05:\n",
    "        vader_label = \"negative\"\n",
    "    else:\n",
    "        vader_label = \"neutral\"  # Ensure consistency with manual labels\n",
    "\n",
    "    return vader_label == label\n",
    "\n",
    "df['vader_match'] = df.apply(lambda row: sentiment_match(row['vader_score'], row['Sentiment']), axis=1)\n",
    "\n",
    "match_rate = df['vader_match'].mean()\n",
    "print(f\"VADER matches manual labels {match_rate*100:.2f}% of the time.\")\n",
    "\n",
    "\n",
    "df['Sentiment'] = df['Sentiment'].str.strip().str.lower()  # Remove spaces & lowercase\n",
    "candidate_mapping = {'positive': 1, 'negative': -1, 'neutral': 0}\n",
    "df['Sentiment'] = df['Sentiment'].map(candidate_mapping)\n",
    "df['Sentiment'] = df['Sentiment'].fillna(1).astype(int)\n",
    "df = pd.get_dummies(df, columns=['Candidate'], prefix='Candidate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "cc14588b-57ca-4787-97e4-a855641824e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced df shape: (500, 32)\n",
      "df_model.shape=(500, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullText</th>\n",
       "      <th>day</th>\n",
       "      <th>July</th>\n",
       "      <th>August</th>\n",
       "      <th>September</th>\n",
       "      <th>October</th>\n",
       "      <th>November</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>views</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>n_unique_words</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>n_negations</th>\n",
       "      <th>textblob_polarity</th>\n",
       "      <th>n_emojis</th>\n",
       "      <th>n_support_words</th>\n",
       "      <th>n_attack_words</th>\n",
       "      <th>engagement_tier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i think what people aren’t taking about enough...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>4.454545</td>\n",
       "      <td>31</td>\n",
       "      <td>179</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.216667</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can’t believe trump is really launching a pr...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>...</td>\n",
       "      <td>4.777778</td>\n",
       "      <td>9</td>\n",
       "      <td>51</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trump on the radio show sid &amp; friends in the m...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>...</td>\n",
       "      <td>4.207547</td>\n",
       "      <td>48</td>\n",
       "      <td>275</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0.389286</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>most of these trump supporting men are lonely ...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>4.451613</td>\n",
       "      <td>30</td>\n",
       "      <td>169</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.091071</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i think weve all confused jerry brown with wil...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>...</td>\n",
       "      <td>4.357143</td>\n",
       "      <td>36</td>\n",
       "      <td>224</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.277778</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            fullText  day  July  August  \\\n",
       "0  i think what people aren’t taking about enough...   14     0       1   \n",
       "1  i can’t believe trump is really launching a pr...   13     0       0   \n",
       "2  trump on the radio show sid & friends in the m...    7     0       0   \n",
       "3  most of these trump supporting men are lonely ...   21     0       0   \n",
       "4  i think weve all confused jerry brown with wil...    9     0       1   \n",
       "\n",
       "   September  October  November     likes  retweets     views  ...  \\\n",
       "0          0        0         0  0.000025  0.000010  0.000023  ...   \n",
       "1          0        1         0  0.000208  0.000052  0.000139  ...   \n",
       "2          0        1         0  0.000024  0.000062  0.000054  ...   \n",
       "3          1        0         0  0.000021  0.000010  0.000003  ...   \n",
       "4          0        0         0  0.000178  0.000146  0.000071  ...   \n",
       "\n",
       "   avg_word_length  n_unique_words  text_length word_count  n_negations  \\\n",
       "0         4.454545              31          179         33            1   \n",
       "1         4.777778               9           51          9            0   \n",
       "2         4.207547              48          275         53            1   \n",
       "3         4.451613              30          169         31            0   \n",
       "4         4.357143              36          224         42            0   \n",
       "\n",
       "   textblob_polarity  n_emojis  n_support_words  n_attack_words  \\\n",
       "0          -0.216667         2                1               0   \n",
       "1           0.200000         1                0               1   \n",
       "2           0.389286        11                1               0   \n",
       "3          -0.091071         0                1               0   \n",
       "4          -0.277778         2                0               1   \n",
       "\n",
       "   engagement_tier  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                2  \n",
       "4                1  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################\n",
    "# STEP 1: Process the dataset\n",
    "########################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "# 1. Linguistic & Lexical Features\n",
    "df['avg_word_length'] = df['fullText'].apply(lambda x: np.mean([len(w) for w in x.split()]) if x.split() else 0)\n",
    "df['n_unique_words'] = df['fullText'].apply(lambda x: len(set(x.split())))\n",
    "df['text_length'] = df['fullText'].apply(len)\n",
    "df['word_count'] = df['fullText'].apply(lambda x: len(x.split()))\n",
    "df['n_negations'] = df['fullText'].str.count(r'\\b(not|no|never|don\\'t|doesn\\'t|can\\'t|won\\'t|isn\\'t|ain\\'t)\\b')\n",
    "\n",
    "# 2. Tone & Emotion Features\n",
    "df['textblob_polarity'] = df['fullText'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "df['n_emojis'] = df['fullText'].str.count(r'[^\\w\\s,]')\n",
    "\n",
    "# 3. Political Lexicon Features\n",
    "support_words = ['vote', 'support', 'endorse', 'leader', 'elect', '💙']\n",
    "attack_words = ['fraud', 'corrupt', 'criminal', 'lie', 'scandal', 'rigged', 'communist']\n",
    "\n",
    "df['n_support_words'] = df['fullText'].apply(lambda x: sum(word in x.lower() for word in support_words))\n",
    "df['n_attack_words'] = df['fullText'].apply(lambda x: sum(word in x.lower() for word in attack_words))\n",
    "\n",
    "# 4. Engagement Tier (optional — if engagement_rate exists)\n",
    "if 'engagement_rate' in df.columns:\n",
    "    df['engagement_tier'] = pd.qcut(df['engagement_rate'], q=4, labels=False, duplicates='drop')  # 0 = low, 3 = viral\n",
    "\n",
    "# 5. Optional: Direction-Sentiment Conflict Feature\n",
    "# (Only useful if you have 'direction' labels and want to model indirect/direct)\n",
    "\n",
    "# Done! Check shape and sample output\n",
    "print(\"Enhanced df shape:\", df.shape)\n",
    "df.head(3)\n",
    "\n",
    "\n",
    "df_model = df.drop(columns = ['vader_match', 'X', 'candidate_mentioned'])\n",
    "\n",
    "print(f\"df_model.shape={df_model.shape}\")\n",
    "\n",
    "df_model.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "94c5336a-803d-4538-a88f-535f17272df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final x_train shape: (500, 525)\n",
      "Final y_train shape: (500,)\n",
      "    10      2016  2020  2024  2025  abortion     about  absolutely  actually  \\\n",
      "0  0.0  0.269745   0.0   0.0   0.0       0.0  0.137932         0.0       0.0   \n",
      "1  0.0  0.000000   0.0   0.0   0.0       0.0  0.000000         0.0       0.0   \n",
      "2  0.0  0.000000   0.0   0.0   0.0       0.0  0.000000         0.0       0.0   \n",
      "3  0.0  0.000000   0.0   0.0   0.0       0.0  0.157115         0.0       0.0   \n",
      "4  0.0  0.000000   0.0   0.0   0.0       0.0  0.000000         0.0       0.0   \n",
      "\n",
      "   administration  ...  avg_word_length  n_unique_words  text_length  \\\n",
      "0             0.0  ...         4.454545              31          179   \n",
      "1             0.0  ...         4.777778               9           51   \n",
      "2             0.0  ...         4.207547              48          275   \n",
      "3             0.0  ...         4.451613              30          169   \n",
      "4             0.0  ...         4.357143              36          224   \n",
      "\n",
      "   word_count  n_negations  textblob_polarity  n_emojis  n_support_words  \\\n",
      "0          33            1          -0.216667         2                1   \n",
      "1           9            0           0.200000         1                0   \n",
      "2          53            1           0.389286        11                1   \n",
      "3          31            0          -0.091071         0                1   \n",
      "4          42            0          -0.277778         2                0   \n",
      "\n",
      "   n_attack_words  engagement_tier  \n",
      "0               0                0  \n",
      "1               1                0  \n",
      "2               0                0  \n",
      "3               0                2  \n",
      "4               1                1  \n",
      "\n",
      "[5 rows x 525 columns]\n",
      "0   -1\n",
      "1    0\n",
      "2   -1\n",
      "3   -1\n",
      "4    0\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# STEP 2: Apply \"non-learned\" data transformations\n",
    "########################################\n",
    "vectorizer = TfidfVectorizer(max_features=500)\n",
    "X_model_tfidf = vectorizer.fit_transform(df_model['fullText'])\n",
    "\n",
    "# Step 3: Convert to DataFrame\n",
    "tfidf_train_df = pd.DataFrame(X_model_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Step 4: Remove 'fullText' and 'sentiment' before merging\n",
    "df_model_features = df_model.drop(columns=['fullText', 'Sentiment', 'direction', 'id'])\n",
    "\n",
    "# Step 5: Ensure alignment by resetting index\n",
    "x_train = pd.concat([tfidf_train_df.reset_index(drop=True), df_model_features.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Step 6: Define Target Variable\n",
    "y_train = df_model['Sentiment'].reset_index(drop=True)\n",
    "\n",
    "# Step 7: Verify Final Data\n",
    "print(\"Final x_train shape:\", x_train.shape)  # Should match (90, 1018) if 18 + 1000 features\n",
    "print(\"Final y_train shape:\", y_train.shape)\n",
    "print(x_train.head())  # or print(df_train.head()) to check your dataset\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "12ca854f-c043-4241-8014-6b83cc74fc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Training Class Distribution:\n",
      " Sentiment\n",
      "-1    0.582353\n",
      " 1    0.244118\n",
      " 0    0.173529\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "New Validation Class Distribution:\n",
      " Sentiment\n",
      "-1    0.583333\n",
      " 1    0.250000\n",
      " 0    0.166667\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "New Test Class Distribution:\n",
      " Sentiment\n",
      "-1    0.59\n",
      " 1    0.24\n",
      " 0    0.17\n",
      "Name: proportion, dtype: float64\n",
      "Training set: X_train=(340, 525), Y_train=(340,)\n",
      "Validation set: X_val=(60, 525), Y_val=(60,)\n",
      "Test set: X_test=(100, 525), Y_test=(100,)\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# STEP 3: Create train/test sets\n",
    "########################################\n",
    "train_ratio = 0.70\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=43, stratify=y_train  # Maintains class balance\n",
    ")\n",
    "\n",
    "# Further split training into train/validation\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_train, Y_train, test_size=0.15, random_state=43, stratify=Y_train\n",
    ")\n",
    "\n",
    "# Print new distributions\n",
    "print(\"New Training Class Distribution:\\n\", Y_train.value_counts(normalize=True))\n",
    "print(\"\\nNew Validation Class Distribution:\\n\", Y_val.value_counts(normalize=True))\n",
    "print(\"\\nNew Test Class Distribution:\\n\", Y_test.value_counts(normalize=True))\n",
    "\n",
    "# Print final shapes\n",
    "print(f\"Training set: X_train={X_train.shape}, Y_train={Y_train.shape}\")\n",
    "print(f\"Validation set: X_val={X_val.shape}, Y_val={Y_val.shape}\")\n",
    "print(f\"Test set: X_test={X_test.shape}, Y_test={Y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "637fb7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape=(500, 525)\n",
      "x_train.shape=(500, 525)\n"
     ]
    }
   ],
   "source": [
    "standardize = sklearn.preprocessing.StandardScaler(\n",
    "    with_mean=True,\n",
    "    with_std=True,\n",
    "    )\n",
    "standardize.fit(x_train) # all of these feature transformations have; => \"learned\"\n",
    "x_train = standardize.transform(x_train)\n",
    "X_train = standardize.transform(X_train)\n",
    "X_test = standardize.transform(X_test)\n",
    "X_val = standardize.transform(X_val)\n",
    "print(f\"x_train.shape={x_train.shape}\")\n",
    "\n",
    "# scale the data to a finite range\n",
    "scaler = sklearn.preprocessing.MaxAbsScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)\n",
    "print(f\"x_train.shape={x_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5810a962-e208-4975-97e9-064576f8fdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8000\n",
      "Train Accuracy: 0.9853\n",
      "\n",
      "Train Set Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.99      0.98      0.99       198\n",
      "           0       1.00      1.00      1.00        59\n",
      "           1       0.95      0.99      0.97        83\n",
      "\n",
      "    accuracy                           0.99       340\n",
      "   macro avg       0.98      0.99      0.99       340\n",
      "weighted avg       0.99      0.99      0.99       340\n",
      "\n",
      "\n",
      "Validation Set Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.81      0.86      0.83        35\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       0.62      0.53      0.57        15\n",
      "\n",
      "    accuracy                           0.80        60\n",
      "   macro avg       0.81      0.80      0.80        60\n",
      "weighted avg       0.79      0.80      0.80        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# STEP 5: Train a model\n",
    "########################################\n",
    "\n",
    "model1 = sklearn.tree.DecisionTreeClassifier(\n",
    "   criterion='entropy',\n",
    "   max_depth=2,\n",
    "   min_samples_split=30,\n",
    "   min_samples_leaf=3,\n",
    "   max_features=None,\n",
    "   max_leaf_nodes=5,\n",
    "   random_state=42)\n",
    " \n",
    "sentiment_model = sklearn.ensemble.AdaBoostClassifier(\n",
    "   estimator=model1,\n",
    "   n_estimators=55)\n",
    "\n",
    "\n",
    "sentiment_model.fit(X_train, Y_train)\n",
    "\n",
    "# most of our discussions in class about \"error\"\n",
    "# accuracy is just 1 - error\n",
    "\n",
    "# Report accuracy scores\n",
    "validation_accuracy = sentiment_model.score(X_val, Y_val)\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.4f}\")\n",
    "\n",
    "train_accuracy = sentiment_model.score(X_train, Y_train)\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "y_train_pred = sentiment_model.predict(X_train)\n",
    "print(\"\\nTrain Set Metrics:\")\n",
    "print(classification_report(Y_train, y_train_pred))\n",
    "\n",
    "y_val_pred = sentiment_model.predict(X_val)\n",
    "print(\"\\nValidation Set Metrics:\")\n",
    "print(classification_report(Y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "17acbe13-fae1-4c48-834b-35e9aa46b679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8100\n",
      "Test Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.82      0.86      0.84        59\n",
      "           0       1.00      1.00      1.00        17\n",
      "           1       0.62      0.54      0.58        24\n",
      "\n",
      "    accuracy                           0.81       100\n",
      "   macro avg       0.81      0.80      0.81       100\n",
      "weighted avg       0.80      0.81      0.81       100\n",
      "\n",
      "Unique predictions in y_test_pred: [-1  0  1]\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# STEP 6: Evaluate on test set\n",
    "########################################\n",
    "\n",
    "# WARNING:\n",
    "# this code should be run only once;\n",
    "# after the hyperparameters have been decided based on the validation performance,\n",
    "# then the False can be changed to True to run this code\n",
    "if True:\n",
    "    sentiment_model.fit(X_train, Y_train)\n",
    "    y_test_pred = sentiment_model.predict(X_test)\n",
    "\n",
    "    test_accuracy = sentiment_model.score(X_test, Y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    print(\"Test Report:\")\n",
    "    print(classification_report(Y_test, y_test_pred))\n",
    "    print(\"Unique predictions in y_test_pred:\", np.unique(y_test_pred))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
